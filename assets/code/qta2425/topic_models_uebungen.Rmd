```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require('tidyverse')) install.packages("tidyverse")                      # wrangling
if (!require('quanteda')) install.packages("quanteda")                        # quanteda basic functions
if (!require('stm')) install.packages("stm")                                  # structural topic models

#install.packages("devtools")
#devtools::install_github("matthewjdenny/preText")
library(preText)
```

Nun habt ihr selbst mithilfe eines "The Guardian"-Datensatzes die Möglichkeit ein Topic Model durchzuführen. 
```{r}
df <- readRDS("guardian_corp.RDS")
df$id <- 1:nrow(df)
```


## 1. Schritt: Vorbereitung

Schaut euch den Datensatz genauer an. Welche Spalte ist für euch interessant? 
Lasst Euch ein paar Texte anzeigen, um ein Gefühl für den Datensatz zu erhalten.

```{r}
summary(df)

df$full_text[1:5]
```


Wandelt die Textspalte nun schrittweise in ein dfm-Objekt um. Nimmt dabei eurer Meinung nach nötige Vorbereitungsschritte vor.

```{r}
corp <- corpus(df$full_text)
toks <- tokens(corp,
               remove_punct = T, 
               remove_symbols = T)
toks <- tokens_remove(toks, c(quanteda::stopwords(language="en")))

dfm_guardian <- dfm(toks, tolower=T) 
dfm_guardian
```


Definiert im nächsten Schritt Meta-Variablen als docvars, die für euch interessant sein könnten.

```{r}
docvars(dfm_guardian, "id") <- df$id
docvars(dfm_guardian, "date") <- df$date
```


Löscht Dokumente, welche nun leer sind aus dem Datensatz sowie aus dem dfm.

```{r}
# drop ids 
drop <- dfm_guardian@docvars$docname_[ntoken(dfm_guardian)==0] %>% str_extract(., "\\d{1,}") %>% as.numeric()

# aus dem data.frame löschen
df <- df[!row.names(df) %in% drop, ]

# aus dfm löschen
dfm_guardian <- dfm_subset(dfm_guardian, ntoken(dfm_guardian)>0)
dfm_guardian
```

Dieser Datensatz ist schon recht groß. Für Lehrzwecke können wir zufällig 15% der Dokumente auswählen, um die Bearbeitungszeit etwas zu verkürzen. 

```{r}
set.seed(3145)
dfm_guardian_sub <- dfm_sample(dfm_guardian, size=0.15*ndoc(corp))
```


## 2. Analyse

Wie viele Topics sind statistisch gesehen ideal?

```{r}
start <- Sys.time()
# dfm in stm umwandeln
stm_guardian <- convert(dfm_guardian_sub, "stm")

# hier definieren wir mehrere k (großer Datensatz)
K <- seq(5, 60, 5) 

# search for optimal k 
best_k <- searchK(stm_guardian$documents, stm_guardian$vocab, K, seed=532, emtol=0.001, init.type = "Spectral")
end <- Sys.time()
end - start
saveRDS(best_k, "./searchK_guardian.RDS")

plot(best_k)
```


Schätzt ein Topic Model mit der statistisch idealen Anzahl an Topics *oder* der Anzahl, welche ihr für am besten haltet. 

```{r}
stm1 <- stm(dfm_guardian_sub, K = 25, seed=125, init.type = "Spectral")

saveRDS(stm1, "stm_guardian.RDS")
```


## 3. Interpretation

Lasst euch wichtige Keywords zu den jeweiligen Themen ausgeben.

```{r}
labelTopics(stm1)
```

Lassen sich die Themen gut interpretieren? Schaut euch dazugehörige Texte an, wenn ihr euch unsicher seid. 

```{r}
findThoughts(stm1, df_sub$full_text,topics=2, n=5)
```

## 4. Rückkopplung mit Datensatz

Fügt die Themen zum bestehenden Datensatz hinzu.

```{r}

```


Plottet ein Thema eurer Wahl im Zeitverlauf.
```{r}

``` 