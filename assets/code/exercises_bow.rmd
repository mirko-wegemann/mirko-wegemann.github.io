```{r setup,warning=F,message=F}
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(stm)
library(caret)
```


The credits for these exercises go to Theresa Gessler!


For these exercises, we will analyse the theses' abstracts we downloaded yesterday. I downloaded all the abstracts, so please use the file *theses_eui_complete.RDS*. 
There are a few theses for which the Department was wrongly retrieved. Others do not have any abstract. Let's drop these.
```{r}
df <- readRDS("./theses_eui_complete.RDS")

df <- df %>%
  filter(dept %in% c("ECO", "SPS", "LAW", "HEC")) %>%
  filter(!is.na(abstract))
```


In a first step, do all the pre-processing steps you think are important. Also add the dept variable as a docvar to our dfm. 
```{r}
corp_eui <- corpus(df$abstract)
dfm_eui <- tokens(corp_eui,
                  remove_punct=T,
                  remove_symbols=T,
                  split_hyphens=T) %>%
  tokens_remove(stopwords("en")) %>%
  dfm()

dfm_eui

dfm_eui <- dfm_subset(dfm_eui, ntoken(dfm_eui)>0)
docvars(dfm_eui, "dept") <- df$dept
```


## Running a topic model
Next, it would be nice to know more about the different topics the theses cover. Run a structured topic model. Choose as many topics k you think are appropriate. Don't forget to set a seed to replicate your results later! It may take a while. So during waiting time, you can continue writing the syntax for labeling the topics and showing plotting a graph
```{r}
stm_eui <- stm(dfm_eui, K = 20, seed=62, emtol=0.001)
```

So, now show the words which are associated to each topic. 
```{r}
labelTopics(stm_eui)
```

Please briefly think of the topics - do they make sense? What could you do to improve them? 

Then, pick one topic and plot one corresponding abstract. 
```{r}
findThoughts(stm_eui,
             df$abstract,topics=10,n=1)
```

Are you happy with the model? 

*Advanced*

This takes time!
If you want, use the searchK algorithm to try out different numbers of k. Use the best model according to these technical stats, estimate it and compare it to your initial model (be careful, don't overwrite the original model, use new variable names!)
```{r}
start <- Sys.time()
set.seed(523)
stm_eui <- convert(dfm_eui,"stm")

K<- seq(4,20,3)

# run validation
k_thesis <- searchK(stm_eui$documents, stm_eui$vocab, K)
end <- Sys.time()
end-start
saveRDS(k_thesis, "./searchK_theses.RDS")

plot(k_thesis)
```


Now, if you chose your favorite model, it would be interesting to actually know which topic is most prevalent in which Department. Use stm's prevalence covariate function to estimate a new topic model. 
```{r}
stm_eui2 <- stm(dfm_eui, K = 12, seed=812, emtol=0.001, prevalence=~dept, data=docvars(dfm_eui))
labelTopics(stm_eui2)
```

Choose one topic which interests you and compare the prevalence within the Departments.
```{r}
prev_dept <- estimateEffect(c(8) ~ dept, stm_eui2, docvars(dfm_eui))

summary(prev_dept)
plot(prev_dept,topics=8,covariate="dept")
```


## Training the classifier
If you want, you can also try to classify these abstracts into different Departments.

So, first: set a seed and divide the thesis into a training and test split. Make sure the dfm are of the same length. 
```{r}
set.seed(521)

train <- dfm_sample(dfm_eui,0.8*nrow(df))
test <- dfm_subset(dfm_eui,
  !(docnames(dfm_eui) %in% docnames(train)))
train <- train %>% dfm_trim(1)
test <- dfm_match(test, featnames(train))
```


Using the `dept` document variable, train a Naive Bayes and a SVM model to predict the department based on an abstract.
```{r}
model_nb <- textmodel_nb(train,docvars(train,"dept"))
model_svm <- textmodel_svm(train,docvars(train,"dept"))
```


Now, predict the test data with the models you just trained. 
```{r}
test_predictions_nb <-predict(model_nb, newdata=test)
test_predictions_svm <-predict(model_svm, newdata=test)
```

How well does the prediction perform? 
```{r}
confusionMatrix(test_predictions_nb, as.factor(docvars(test,"dept")))
confusionMatrix(test_predictions_svm, as.factor(docvars(test,"dept")))
```
Based on the results, which classifier performs better?